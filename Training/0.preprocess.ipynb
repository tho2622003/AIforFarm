{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Đang xử lý nồng độ 250500 =====\n",
      "Số dòng ban đầu: 11839\n",
      "Số dòng sau khi loại trùng lặp: 11839\n",
      "Các cột bị loại bỏ do quá nhiều thiếu/0: ['H2S_concentration', 'CH4ppm', 'HCHOmg/m3', 'TVOCppm']\n",
      "Đã xuất dữ liệu đã tiền xử lý ra: E:\\Khóa luận\\Exports\\Processed\\250500_processed.csv\n",
      "\n",
      "===== Đang xử lý nồng độ 500500 =====\n",
      "Số dòng ban đầu: 8646\n",
      "Số dòng sau khi loại trùng lặp: 8646\n",
      "Các cột bị loại bỏ do quá nhiều thiếu/0: ['ccsTVOC', 'H2S_concentration', 'CH4ppm', 'HCHOmg/m3', 'TVOCppm']\n",
      "Các cột bị loại bỏ sau differencing: ['ccseCO2']\n",
      "Đã xuất dữ liệu đã tiền xử lý ra: E:\\Khóa luận\\Exports\\Processed\\500500_processed.csv\n",
      "\n",
      "===== Đang xử lý nồng độ 750500 =====\n",
      "Số dòng ban đầu: 26315\n",
      "Số dòng sau khi loại trùng lặp: 26315\n",
      "Các cột bị loại bỏ do quá nhiều thiếu/0: ['ccsTVOC', 'H2S_concentration', 'CH4ppm', 'HCHOmg/m3', 'TVOCppm']\n",
      "Các cột bị loại bỏ sau differencing: ['ccseCO2']\n",
      "Đã xuất dữ liệu đã tiền xử lý ra: E:\\Khóa luận\\Exports\\Processed\\750500_processed.csv\n",
      "\n",
      "Đã xử lý và lưu toàn bộ biểu đồ cho các nồng độ vào: E:\\Khóa luận\\Exports\\Processed\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional, TimeDistributed, RepeatVector\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration\n",
    "base_dir = r'E:\\Khóa luận\\Data\\Phase 2'\n",
    "export_dir = r'E:\\Khóa luận\\Exports\\Processed'\n",
    "os.makedirs(export_dir, exist_ok=True)\n",
    "\n",
    "# Parameters\n",
    "target_col = 'CH4RAWppm'  # Target variable to predict\n",
    "window_size = 72          # Lookback window (3 days at hourly resolution)\n",
    "forecast_horizon = 24     # Predict next 24 hours (1 day)\n",
    "batch_size = 64\n",
    "epochs = 200\n",
    "\n",
    "# Data loading with your preprocessing\n",
    "def load_and_preprocess(file_path):\n",
    "    df = pd.read_csv(file_path, sep=';', encoding='utf-8', low_memory=False)\n",
    "    \n",
    "    # Basic preprocessing\n",
    "    df = df.drop_duplicates()\n",
    "    if 'Date time' in df.columns:\n",
    "        df['Date time'] = pd.to_datetime(df['Date time'], errors='coerce')\n",
    "    \n",
    "    # Handle numerical columns\n",
    "    numerical_cols = [col for col in df.columns if col not in ['Timestamp', 'Date time'] \n",
    "                     and pd.api.types.is_numeric_dtype(pd.to_numeric(df[col], errors='coerce'))]\n",
    "    \n",
    "    for col in numerical_cols:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "        df.loc[df[col] < 0, col] = np.nan\n",
    "        \n",
    "        # Fill missing values\n",
    "        df[col] = df[col].interpolate(method='linear', limit_direction='both')\n",
    "        df[col] = df[col].ffill().bfill()\n",
    "        df[col] = df[col].fillna(df[col].median())\n",
    "    \n",
    "    # Remove columns with >50% missing/zeros\n",
    "    threshold = 0.5\n",
    "    cols_to_drop = [col for col in numerical_cols \n",
    "                   if (df[col].isna().sum()/len(df) > threshold) \n",
    "                   or ((df[col] == 0).sum()/len(df) > threshold]\n",
    "    \n",
    "    if cols_to_drop:\n",
    "        df = df.drop(columns=cols_to_drop)\n",
    "        numerical_cols = [col for col in numerical_cols if col not in cols_to_drop]\n",
    "    \n",
    "    # Outlier handling\n",
    "    for col in numerical_cols:\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower = Q1 - 1.5 * IQR\n",
    "        upper = Q3 + 1.5 * IQR\n",
    "        df.loc[(df[col] < lower) | (df[col] > upper), col] = np.nan\n",
    "        df[col] = df[col].interpolate(method='linear', limit_direction='both')\n",
    "        df[col] = df[col].ffill().bfill()\n",
    "        df[col] = df[col].fillna(df[col].median())\n",
    "    \n",
    "    # Make stationary\n",
    "    for col in numerical_cols:\n",
    "        df[col] = df[col].diff().fillna(0)\n",
    "    \n",
    "    # Final cleanup\n",
    "    cols_all_zero = [col for col in numerical_cols if (df[col] == 0).all()]\n",
    "    if cols_all_zero:\n",
    "        df = df.drop(columns=cols_all_zero)\n",
    "        numerical_cols = [col for col in numerical_cols if col not in cols_all_zero]\n",
    "    \n",
    "    return df, numerical_cols\n",
    "\n",
    "# Sequence creation\n",
    "def create_sequences(data, target, window, horizon):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - window - horizon + 1):\n",
    "        X.append(data[i:i+window])\n",
    "        y.append(target[i+window:i+window+horizon])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Main training loop\n",
    "results = []\n",
    "input_files = {\n",
    "    '250500': os.path.join(base_dir, '250500.csv'),\n",
    "    '500500': os.path.join(base_dir, '500500.csv'),\n",
    "    '750500': os.path.join(base_dir, '750500.csv')\n",
    "}\n",
    "\n",
    "for label, file_path in input_files.items():\n",
    "    print(f\"\\n===== Processing {label} =====\")\n",
    "    \n",
    "    # Load and preprocess\n",
    "    df, numerical_cols = load_and_preprocess(file_path)\n",
    "    \n",
    "    if target_col not in numerical_cols:\n",
    "        print(f\"Target column {target_col} not found in {label}\")\n",
    "        continue\n",
    "    \n",
    "    # Scale data\n",
    "    scaler = StandardScaler()\n",
    "    scaled_values = scaler.fit_transform(df[[target_col]])\n",
    "    \n",
    "    # Create sequences\n",
    "    X, y = create_sequences(scaled_values, scaled_values, window_size, forecast_horizon)\n",
    "    \n",
    "    # Train-test split\n",
    "    split_idx = int(0.8 * len(X))\n",
    "    X_train, y_train = X[:split_idx], y[:split_idx]\n",
    "    X_test, y_test = X[split_idx:], y[split_idx:]\n",
    "    \n",
    "    # Build model\n",
    "    model = Sequential([\n",
    "        Bidirectional(LSTM(128, return_sequences=True), \n",
    "                     input_shape=(window_size, 1)),\n",
    "        Dropout(0.3),\n",
    "        Bidirectional(LSTM(64)),\n",
    "        Dropout(0.2),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(forecast_horizon)\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "                 loss='mse',\n",
    "                 metrics=['mae'])\n",
    "    \n",
    "    # Callbacks\n",
    "    callbacks = [\n",
    "        EarlyStopping(patience=15, restore_best_weights=True),\n",
    "        ModelCheckpoint(os.path.join(export_dir, f'best_model_{label}.h5'), \n",
    "                        save_best_only=True)\n",
    "    ]\n",
    "    \n",
    "    # Train\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_split=0.2,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Evaluate\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Inverse scaling\n",
    "    y_test_orig = np.array([scaler.inverse_transform(seq.reshape(-1, 1)).flatten() \n",
    "                          for seq in y_test])\n",
    "    y_pred_orig = np.array([scaler.inverse_transform(seq.reshape(-1, 1)).flatten() \n",
    "                          for seq in y_pred])\n",
    "    \n",
    "    # Calculate metrics\n",
    "    r2 = r2_score(y_test_orig.flatten(), y_pred_orig.flatten())\n",
    "    mae = mean_absolute_error(y_test_orig.flatten(), y_pred_orig.flatten())\n",
    "    rmse = np.sqrt(mean_squared_error(y_test_orig.flatten(), y_pred_orig.flatten()))\n",
    "    \n",
    "    results.append({\n",
    "        'Concentration': label,\n",
    "        'R2': r2,\n",
    "        'MAE': mae,\n",
    "        'RMSE': rmse\n",
    "    })\n",
    "    \n",
    "    # Visualization\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    for i in range(3):  # Plot 3 samples\n",
    "        plt.subplot(3, 1, i+1)\n",
    "        plt.plot(y_test_orig[i], label='Actual')\n",
    "        plt.plot(y_pred_orig[i], label='Predicted')\n",
    "        plt.title(f'Sample {i+1} - {label}')\n",
    "        plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(export_dir, f'predictions_{label}.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    # Plot training history\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(history.history['loss'], label='Train Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title(f'Training History - {label}')\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(export_dir, f'training_history_{label}.png'))\n",
    "    plt.close()\n",
    "\n",
    "# Save results\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\nFinal Results:\")\n",
    "print(results_df)\n",
    "results_df.to_csv(os.path.join(export_dir, 'model_results.csv'), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
